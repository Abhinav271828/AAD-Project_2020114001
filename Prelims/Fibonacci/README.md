# Algorithms to Compute Fibonacci Numbers
Four algorithms were taught in class to compute the Fibonacci numbers:

1. naïve recursion  
2. memoised dynamic programming  
3. the matrix formula  
4. the √5 formula  

The first two are implemented under the same names. Three variations of the third one are implemented – the first one using naïve matrix exponentiation, the second binary exponentiation, and the third binary exponentiation paired with the Strassen algorithm.  

The fourth algorithm is *not* implemented, since a Haskell module (library) for arbitrary-precision floating-point values was not available. However, it has been discussed in class that this method cannot be better than any of the other three.  

Note: Haskell has an arbitrary-size integer type named `Integer`. This is the return type of all the functions.

## Running
To run one of the algorithms, uncomment (remove the two hyphens and the space preceding) the corresponding line in the `main` function, save the file, and recompile. When running, pass *n* as a command-line argument.
```
> ./fibonacci 100
> 354224848179261915075
```

## Explanation
### Naïve Recursion
This solution mirrors the defining formula of the Fibonacci sequence. Two lines give the base cases and the third line encodes the recursion.

### Memoised Solution
This solution first generates an infinite *list* of the Fibonacci sequence `fibs`, and then indexes into it. The infinite list is generated by the `zipWith` function, which applies an operator (in this case `+`) to each pair of the corresponding elements of two lists (in this case `fibs` and `tail fibs`, which is `fibs` without its first element).  

Haskell's laziness allows the definition of `fibs` to involve itself, as long as the base case is adequately supplied. Here, this is the first two elements of `fibs`, 0 and 1.  

`(!!)` is the list indexing operator in Haskell.

### Naïve Matrix Method
For the following three methods, matrices are implemented using the type `Matr Integer`, which is a nested list.  

This function starts with the base matrix `[0 1]` and multiplies on the left *n* times by `[[0,1],[1,1]]`. The function `appl` applies a given function to a given value, a given number of times.  

Note that here, no true matrix exponentiation is taking place; the only kind of multiplication is of a 2x2 matrix with a 2x1 matrix, by the function `mult`.

### Optimised Matrix Method
This method performs true matrix exponentiation, in a logarithmic fashion: Mⁿ is calculated by squaring the (*n*/2)th power and multiplying by M as needed (according as *n* is odd or even). The multiplication by M is done using 8 operations, the ordinary way, using the function `mul`. Squaring, similarly, takes 5 multiplications.  

The function thus multiplies Mⁿ with `[0,1]` to obtain the result.

### Strassen-Optimised Matrix Method
This function is identical to the above one, except that the multiplication by M (during exponentiation, in the case where *n* is odd) is done in 7 operations by `mul` instead of 8, using the Strassen algorithm.

## Analysis
The R² values of the best fit of various types of functions for each of the algorithms' running times are shown below.  

Best fit curves are not considered for the last two methods as their values show that the trendline is not accurate.

### Naïve Recursion
This is the most inefficient implementation by far. It takes close to 2 minutes to calculate F(45), while the other algorithms' running times remain under 1 second up to several orders of magnitude.  

Clearly, the graph shows rapid exponential growth. When shown in a logarithmic scale, it appears roughly like a line. The exponential best fit has an R² value of 97.4%.  

The predicted complexity of this function is O(F(*n*)) = O(φⁿ). Oberserving the best fit equation, we see that the base is e^(0.361) = 1.42, which is close to φ.  
This correlates with the known fact that F(*n*) tends to φⁿ. The difference can be accounted for by the small sample size.

![Running Time of Naïve Recursion](Naïve\ Recursion.png)
![Running Time of Naïve Recursion (log scale)](Naïve\ Recursion\ (log\ scale).png)

### Memoised Solution
This is the best method apart from the optimised matrix methods.  

We note first that its growth appears mainly linear, although we expect a quadratic growth.  
Two things can be noted in this connection. First, the quadratic best fit has a better R² value (although this is not a very reliable measure of accuracy). Second, Haskell's implementation of arbitrary-size integer arithmetic, although it is linear, is extremely efficient and unlikely to show significant change in the results for numbers in this range (0 to 1,00,000).

![Running Time of the Memoised Solution](Memoised\ Solution.png)

### Naïve Matrix Method
This is the simplest matrix method. It multiplies the base (2x1) matrix on the left with a 2x2 matrix, *n* times. Each of these multiplications takes 2 arbitrary-size integer additions – thus we have an expected running time quadratic in the input.  

However, as noted above, the arbitrary-precision operations do not make much difference in the range taken (0 to 1,25,000). This is why the graph appears almost linear.  
However, it is slightly more noticeably curved than the memoised solution's graph. This could be because (i) the actual number of operations (2*n*) is more than than in the memoisation algorithm (*n*), and (ii) the range is bigger. W.r.t (ii), if we stop the naïve matrix method graph at *n* = 1,00,000, it appears much closer to a linear trend (the curvature becomes noticeable only after this threshold).

![Running Time of the Naïve Matrix Method](Naïve\ Matrix\ Method.png)

### Optimised Matrix Method
The following two methods present the most surprising result: there is almost no overall increase, and certainly no regular increase in its running time, from *n* = 0 to *n* = 5,00,000. The only remark is a slight hike around *n* = 3,00,000.  
This rise could be attributed to chance, if it were not for the fact that the Strassen-optimised matrix method exhibits it as well. A possible reason for it is the increased time for arbitrary-precision integer arithmetic.  

A theoretical analysis of the number of operations involved would be instructive. Let us suppose we are trying to compute F(*n*).  
During the matrix exponentiation (which is the only part dependent on *n*), if *n* is odd, a multiplication and a squaring takes place; and if n is even, only a squaring takes place. This is on top of exponentiation to the power of (*n*/2).  

It is clear, then, that log(*n*) squaring operations take place. Further, a quick analysis shows that if w(x) represents the number of ones in the binary representation of x, then w(*n*)-1 multiplications are performed. Thus we have an overall 5log(*n*) + 8(w(*n*) - 1) operations carried out. This is much, much lower than the 2n, n, and φⁿ operations that the previous algorithms require; this accounts for the drastic difference.  

Another observation we might make is that since the running time is not increasing significantly from *n* = 0 to *n* = 5,00,000, it must be almost negligible. This tells us that the absolute minimum time that the algorithms can take (including time for file access, function calling, parsing, etc.) is roughly 0.03-0.04 seconds. This is borne out most strongly by the graph of the memoised solution – it stays in this approximate range until *n* = 30,000, and only then begins to significantly increase.

![Running Time of the Optimised Matrix Method](Optimised\ Matrix\ Method.png)

### Strassen-Optimised Matrix Method
This method presents an almost identical graph to the ordinary optimised matrix method, considered previously (including the rise at *n* = 3,00,000). From this we conclude that using Strassen's algorithm for matrix multiplication does not lead to a significant improvement.  

This is probably because the matrices involved are extremely small (only 2x2) and arbitrary matrix multiplication (as opposed to squaring) takes place only w(*n*)-1 times, which is always lesser than or equal to log(*n*) (and frequently much, much lesser). The expected number of operations is 5log(*n*) + 7(w(*n*) - 1).  

Besides this, no new conclusions can be drawn from this analysis.

![Running Time of the Strassen-Optimised Matrix Method](Strassen-Optimised\ Matrix\ Method.png)
![Comparison of the Two Optimised Matrix Methods](Comparison\ of\ Optimised\ Matrix\ Methods.png)

## Overall Comparisons
We will first compare all the algorithms among themselves. This again illustrates the extreme inefficiency of naïve recursion – it blows beyond 10⁸ μs within one order of magnitude, while the others stay well below 10⁶ μs up to five orders.

![Comparison of All Methods (log scale)](Comparison\ of\ All\ Methods\ (log\ scale).png)

We can, therefore, exclude it from consideration. A bird's-eye view of the graph of the remaining algorithms' runtimes shows that the best method (apart from the optimised matrix methods, which are nearly constant) is the memoised dynamic programming method. It involves fewer (approximately half) arbitrary-size arithmetic operations than the naïve matrix method, even though the latter does not employ expensive matrix multiplication operations.

![Comparison of Efficient Methods](Comparison\ of\ Efficient\ Methods.png)
